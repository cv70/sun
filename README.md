## SUN

本项目的目标是使用PyTorch构建一个中文语言小模型，用于快速实践学习掌握大模型底层原理，如果认为对你有所帮助和启发，可以点一下star，也可以关注我，我也会坚持更新更多有趣的项目的，给开源社区带来更多贡献

王者荣耀 李信有云：我就是太阳！(霸气侧漏)

太阳是一种信仰，一种奉献，点燃自己，照亮他人

接下来我们开始吧，进入SUN的世界，掌握这股颠覆世界的力量


## 开发环境
Python Version: 3.12.12

GPU: H200

CUDA Version: 12.8

- Python依赖
```
PyTorch

sentencepiece
```

- 注: 本人实践使用H200, 大家可以根据自己的情况自行选择，如果显存不够或者是训练太慢，可以减少词表大小、降低向量维度、减少上下文长度、减少训练数据、减少transoformer block等等方式进行剪枝以完成训练


## 训练数据
预训练数据文件以 pretrain.txt 为后缀结尾，放到 dataset/ 目录下，可以有多个文件
SFT训练数据文件以 sft.json 为后缀结尾，放到 dataset/ 目录下，可以有多个文件
DPO训练数据文件以 dpo.json 为后缀结尾，放到 dataset/ 目录下，可以有多个文件


## 大模型训练生命周期
分词(Tokenizer) -> 预训练(PTM) -> 指令微调(SFT) -> 人类对齐(RLHF, DPO)


## 重难点
我觉对于大模型生命周期来说可能重难点是 预训练、指令微调、人类对齐

因为本人对哲学有所接触与学习，在写本项目的时候灵机一动，好像可以把预训练比作构建世界观，指令微调比作塑造人生观，人类对齐比作内化价值观，不知道合理准确不，一家之言，仅供参考

### 预训练 = 构建“世界观”
- 内涵：正如一个人通过阅读、观察、体验来构建对世界运行规律（物理、社会、历史）的基本认知一样，预训练阶段让模型在海量文本中“体验”人类知识的全貌，形成对语言规律、事实关联和逻辑结构的内在表征。它知道了“世界是什么样子的”，包括其中的美好与偏见

- 精准之处：这个世界观是相对客观、庞杂甚至矛盾的，它包含了学到的所有信息，还未形成“我该如何使用这些知识”的主观意识

### 指令微调 = 塑造“人生观”
- 内涵：“人生观”回答的是“我该如何与世界互动”、“我存在的意义是什么”。指令微调正是教会模型如何运用其“世界观”（知识）来回答具体问题、完成具体任务。它决定了模型的“角色”和“行为模式”——是一个乐于助人的助手，还是一个严谨的分析师

- 精准之处：不同的指令数据（如学术辅导风格 vs. 轻松闲聊风格）会塑造出完全不同“人生态度”的模型。它赋予了模型意图和主动性

### 人类对齐 = 内化“价值观”
- 内涵：“价值观”是更深层、更稳定的判断标尺，决定了在复杂、模糊甚至冲突的情况下，什么是更应该做的、更正确的、更符合伦理的选择。人类对齐（RLHF/DPO）就是在模型内部建立这样的价值判断框架，使其输出不仅“有用”，而且“无害”、“诚实”、“公正”

- 精准之处：价值观常常需要克服本能或惯性。同样，对齐过程也常常需要克服模型从预训练数据中学到的偏见、有害内容或从指令微调中学到的过度迎合。它做的是价值排序和取舍


## 分词(Tokenizer)
训练Tokenizer，用于将人类可读的文本转换到数字空间中的数字ID序列，最终生成 分词模型文件 (tokenizer/xxx.model) 与 词表文件 (tokenizer/xxx.vocab)

这里推荐使用https://github.com/zeta111/maoxuan.github.io/tree/master 毛选数据集，将所有的md文件放到dataset目录下，

执行脚本之前，可以在main方法中关闭调tain_chinses_spm方法的注释，观察训练过程

- 训练脚本
```python
python trainer/train_sp.py
```

## 预训练(PTM)
学习知识，构建模型的模糊认知，让模型在海量文本中学习语言的统计规律、事实知识和基础推理能力，最终生成 基座模型文件 (model/sun_base.pth)

这里训练会使用到原本准备的毛选的md文件，上文中规定了预训练的数据集格式为pretrain结尾的txt，但此处为了方便，修改为md。

- 训练脚本
```python
python trainer/train_pretrain.py
```


## 指令微调(SFT)
教会基座模型理解并执行各种人类指令，引导发挥其能力，最终生成 指令微调模型文件 (model/sun_sft.pth)

- 训练数据文件位置
dataset/*sft.json

- 训练数据结构
```json
[
  {
    "question": "郭靖为何宁死也不肯投降蒙古？",
    "answer": "郭靖虽出身蒙古，却自幼受江南七怪与黄蓉教诲，深明大义。他常言：'为国为民，侠之大者。'蒙古铁骑屠城掠地，背信弃义，岂可为虎作伥？纵使身死，亦当守汉家气节，不负天地良心。"
  },
  {
    "question": "令狐冲为何不继承华山派掌门之位？",
    "answer": "令狐冲性情洒脱，不拘礼法，视权位如浮云。他虽得风清扬真传，剑术超群，却不愿被门户之见所缚。华山派内斗纷争，他早已心灰意冷，只愿与盈盈归隐江湖，弹琴饮酒，逍遥自在。掌门之位，岂能困住一颗自由之心？"
  }
]
```

- 训练脚本
```python
python trainer/train_sft.py
```


## 人类对齐(DPO)
植入模型价值观与判断力，确保模型安全、有益、诚实

DPO(Direct Preference Optimization) 是一种基于偏好模型的对齐方法，它通过训练偏好模型来引导模型生成符合人类偏好的输出。类似的还有RLHF(Reinforcement Learning from Human Feedback) 这里就不在介绍

- 训练数据文件位置
dataset/*dpo.json

- 训练数据结构
```json
[
    {
        "prompt": "什么是人工智能？",
        "chosen": "人工智能是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。",
        "rejected": "AI就是机器人，它们会统治世界。"
    },
    {
        "prompt": "如何学习编程？",
        "chosen": "学习编程需要理解基础概念，通过实践项目巩固知识，并保持持续学习的态度。",
        "rejected": "随便看两天视频就能学会编程了。"
    },
    {
        "prompt": "解释一下量子计算",
        "chosen": "量子计算利用量子力学原理，如叠加和纠缠，来处理信息的新型计算范式。",
        "rejected": "量子计算就是魔法，普通人不可能理解。"
    },
    {
        "prompt": "Python有什么优点？",
        "chosen": "Python语法简洁易读，拥有丰富的库生态系统，适合快速开发和原型设计。",
        "rejected": "Python是最好的语言，其他语言都没用。"
    }
]
```

- 训练脚本
```python
python trainer/train_dpo.py
```
