## SUN

本项目的目标是使用PyTorch构建一个中文语言小模型，用于快速实践学习掌握大模型底层原理，如果认为对你有所帮助和启发，可以点一下star，也可以关注我，我也会坚持更新更多有趣的项目的，给开源社区带来更多贡献

王者荣耀 李信有云：我就是太阳！(霸气侧漏)

太阳是一种信仰，一种奉献，点燃自己，照亮他人

接下来我们开始吧，进入SUN的世界，掌握这股颠覆世界的力量


## 开发环境
Python Version: 3.12.12

GPU: H200

CUDA Version: 12.8

- Python依赖
```
PyTorch

sentencepiece
```

- 注: 本人实践使用H200, 大家可以根据自己的情况自行选择，如果显存不够或者是训练太慢，可以减少词表大小、降低向量维度、减少上下文长度、减少训练数据、减少transoformer block等等方式进行剪枝以完成训练


## 训练数据
训练数据以 .txt 后缀结尾，放到 dataset/ 目录下，可以有多个文件


## 大模型生命周期
分词(Tokenizer) -> 预训练(PTM) -> 指令微调(SFT) -> 人类对齐(RLHF, DPO) -> 测评 -> 部署


## 重难点
我觉对于大模型生命周期来说可能重难点是 预训练、指令微调、人类对齐

因为本人对哲学有所接触与学习，在写本项目的时候灵机一动，好像可以把预训练比作构建世界观，指令微调比作塑造人生观，人类对齐比作内化价值观，不知道合理准确不，一家之言，仅供参考

### 预训练 = 构建“世界观”
- 内涵：正如一个人通过阅读、观察、体验来构建对世界运行规律（物理、社会、历史）的基本认知一样，预训练阶段让模型在海量文本中“体验”人类知识的全貌，形成对语言规律、事实关联和逻辑结构的内在表征。它知道了“世界是什么样子的”，包括其中的美好与偏见

- 精准之处：这个世界观是相对客观、庞杂甚至矛盾的，它包含了学到的所有信息，还未形成“我该如何使用这些知识”的主观意识

### 指令微调 = 塑造“人生观”
- 内涵：“人生观”回答的是“我该如何与世界互动”、“我存在的意义是什么”。指令微调正是教会模型如何运用其“世界观”（知识）来回答具体问题、完成具体任务。它决定了模型的“角色”和“行为模式”——是一个乐于助人的助手，还是一个严谨的分析师

- 精准之处：不同的指令数据（如学术辅导风格 vs. 轻松闲聊风格）会塑造出完全不同“人生态度”的模型。它赋予了模型意图和主动性

### 人类对齐 = 内化“价值观”
- 内涵：“价值观”是更深层、更稳定的判断标尺，决定了在复杂、模糊甚至冲突的情况下，什么是更应该做的、更正确的、更符合伦理的选择。人类对齐（RLHF/DPO）就是在模型内部建立这样的价值判断框架，使其输出不仅“有用”，而且“无害”、“诚实”、“公正”

- 精准之处：价值观常常需要克服本能或惯性。同样，对齐过程也常常需要克服模型从预训练数据中学到的偏见、有害内容或从指令微调中学到的过度迎合。它做的是价值排序和取舍


## 分词(Tokenizer)
训练Tokenizer，用于将人类可读的文本转换到数字空间中的数字ID序列，最终生成 分词模型文件 (tokenizer/xxx.model) 与 词表文件 (tokenizer/xxx.vocab)

- 训练脚本
```python
python trainer/train_sp.py
```


## 预训练(PTM)
学习知识，构建模型的模糊认知，让模型在海量文本中学习语言的统计规律、事实知识和基础推理能力，最终生成 基座模型文件 (model/sun_base.pth)

- 训练脚本
```python
python trainer/train_pretrain.py
```


## 指令微调(SFT)
教会基座模型理解并执行各种人类指令，引导发挥其能力


## 人类对齐(RLHF, DPO)
植入模型价值观与判断力，确保模型安全、有益、诚实


## 测评


## 部署
