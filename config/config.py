"""
=============================================================================
配置文件 (config.py)
=============================================================================

【模块作用】
这个文件定义了整个大语言模型的配置参数字典 LLM_CONFIG。
配置文件是项目的"中央控制台"，集中管理模型架构的所有超参数，
使得修改模型设置时只需更改一处，方便实验和调试。

【相关知识】
1. 字典 (dict): Python中的一种数据结构，使用键值对存储数据
2. 超参数 (Hyperparameters): 模型训练前需要设置的参数，不是通过学习得到的
3. 词表大小 (Vocabulary Size): 模型能识别的不同token的总数
4. 上下文长度 (Context Length): 模型能处理的最大序列长度
5. 嵌入维度 (Embedding Dimension): 将token转换为向量的维度大小
6. 注意力头数 (Number of Heads): 多头注意力机制中头的数量
7. 层数 (Number of Layers): Transformer块的堆叠层数
8. Dropout率: 防止过拟合的正则化技术，随机丢弃一部分神经元
"""

# ============================================================================
# 大语言模型配置字典
# ============================================================================
LLM_CONFIG = {
    # 数据目录：存放训练数据集的文件夹路径
    # 相对路径，指向dataset文件夹
    "data_dir": "dataset",

    # 分词器模型路径：SentencePiece训练的BPE模型文件名
    # BPE (Byte Pair Encoding) 是一种子词分词算法
    "tokenizer_path": "spm_16384.model",

    # 词表大小：16384表示模型能识别16384个不同的token
    # 相关知识：
    # - token是文本的最小单位，可以是词、子词或字符
    # - 词表越大，模型表达能力越强，但参数量也会增加
    # - 16384 (2^14) 是一个中等大小的词表，适合中文模型
    "vocab_size": 16384,

    # 上下文长度：256表示模型一次最多处理256个token
    # 相关知识：
    # - 也称为序列长度或窗口大小
    # - 限制模型能"看到"的历史token数量
    # - 较大的上下文长度能处理更长的文本，但计算量呈平方增长
    # - 256是一个较小的值，适合快速实验和教学
    "context_length": 256,

    # 嵌入维度：256表示每个token被转换为256维的向量
    # 相关知识：
    # - 嵌入是将离散的token ID映射为连续的向量表示
    # - 维度越大，能编码的信息越多，但参数量也越大
    # - 通常设置为词表大小的1/4到1/2之间
    "emb_dim": 256,

    # 注意力头数：8表示多头注意力机制使用8个头
    # 相关知识：
    # - 多头注意力允许模型在不同表示子空间中同时关注信息
    # - 每个头独立计算注意力，最后合并结果
    # - 头数必须能整除嵌入维度 (256 % 8 == 0)
    # - 每个头的维度 = emb_dim / n_heads = 256 / 8 = 32
    "n_heads": 8,

    # Transformer层数：8表示堆叠8个Transformer块
    # 相关知识：
    # - 每一层包含一个自注意力模块和一个前馈神经网络
    # - 层数越多，模型表达能力越强，能学习更复杂的模式
    # - 但也更难训练，容易出现梯度消失/爆炸
    # - GPT-3有96层，BERT-base有12层
    "n_layers": 8,

    # Dropout率：0.1表示训练时随机丢弃10%的神经元输出
    # 相关知识：
    # - Dropout是一种正则化技术，防止模型过拟合
    # - 训练时按概率随机将部分神经元输出置为0
    # - 测试时不使用dropout，使用所有神经元
    # - 0.1是常用的dropout率值
    "drop_rate": 0.1,

    # Query-Key-Value偏置：False表示不使用偏置项
    # 相关知识：
    # - 在计算Q、K、V时是否添加偏置向量
    # - 原始Transformer论文使用bias=True
    # - GPT-2和GPT-3使用bias=False以减少参数量
    # - 设置为False可以节省内存并略微加速计算
    "qkv_bias": False
}
