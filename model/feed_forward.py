"""
=============================================================================
前馈神经网络模块 (feed_forward.py)
=============================================================================

【模块作用】
这个文件实现了Transformer架构中的两个关键组件：
1. FeedForward: 前馈神经网络，对每个token独立应用非线性变换
2. LayerNorm: 层归一化，稳定训练过程

【相关知识】
1. 前馈神经网络 (FFN/MLP): 多层感知机，在Transformer中用于特征变换
2. GELU激活函数: 比ReLU更平滑的激活函数，大模型中常用
3. Layer Normalization: 归一化技术，加速训练并提高稳定性
4. 可学习参数 (Learnable Parameters): 通过训练自动调整的参数
5. 位置独立: FFN对每个token独立处理，不考虑位置关系
"""

# ============================================================================
# 导入必要的库
# ============================================================================
import torch  # PyTorch深度学习框架
import torch.nn as nn  # PyTorch神经网络模块


# ============================================================================
# FeedForward类：前馈神经网络
# ============================================================================
class FeedForward(nn.Module):
    """
    前馈神经网络（也称为位置级前馈网络或MLP）

    相关知识：
    - 在Transformer中，每个位置(token)独立地经过相同的FFN
    - FFN扩展维度到4倍，然后再压缩回来
    - 这种扩展-压缩的结构能让模型学习更复杂的特征
    - 通常结构为: Linear -> GELU -> Linear
    """

    def __init__(self, cfg):
        """
        初始化前馈神经网络

        参数:
            cfg: 配置字典，包含emb_dim等超参数

        相关知识：
        - 标准Transformer使用4倍扩展的隐藏层
        - 例如输入256维，隐藏层1024维，输出256维
        """
        # 调用父类构造函数
        super().__init__()

        # ========================================================================
        # 创建前馈网络层序列
        # ========================================================================
        # nn.Sequential按顺序执行多个层
        self.layers = nn.Sequential(
            # 第一层线性变换：将维度扩展到4倍
            # nn.Linear(in_features, out_features)
            # 例如: emb_dim=256 -> 1024
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),

            # GELU激活函数：引入非线性
            # GELU (Gaussian Error Linear Unit) 比ReLU更平滑
            # 公式: x * Φ(x), 其中Φ是标准正态分布的CDF
            nn.GELU(),

            # 第二层线性变换：将维度压缩回原始大小
            # 例如: 1024 -> 256
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        """
        前向传播

        参数:
            x: 输入张量，形状为(batch_size, seq_len, emb_dim)

        返回:
            输出张量，形状为(batch_size, seq_len, emb_dim)

        相关知识：
        - FFN对每个token独立应用相同的变换
        - 不涉及token之间的交互（由注意力层负责）
        """
        # 按顺序执行所有层
        # self.layers包含三个操作: Linear -> GELU -> Linear
        return self.layers(x)


# ============================================================================
# LayerNorm类：层归一化
# ============================================================================
class LayerNorm(nn.Module):
    """
    层归一化 (Layer Normalization)

    相关知识：
    - LayerNorm是一种归一化技术，用于稳定和加速神经网络训练
    - 与BatchNorm不同，LayerNorm对每个样本独立归一化
    - 在Transformer中，LayerNorm被广泛使用
    - 归一化公式: output = scale * (x - mean) / sqrt(var + eps) + shift
    - scale和shift是可学习参数
    """

    def __init__(self, emb_dim):
        """
        初始化层归一化

        参数:
            emb_dim: 要归一化的特征维度

        相关知识：
        - eps是一个很小的数，防止除以0
        - scale和shift是可学习参数，允许模型学习合适的归一化范围
        """
        # 调用父类构造函数
        super().__init__()

        # ========================================================================
        # 定义可学习参数
        # ========================================================================
        # eps (epsilon): 一个很小的常数，防止方差为0时除以0
        self.eps = 1e-5

        # scale (gamma): 缩放参数，可学习
        # nn.Parameter将张量包装为可训练参数
        # 初始化为全1，表示初始时不改变归一化后的值
        self.scale = nn.Parameter(torch.ones(emb_dim))

        # shift (beta): 平移参数，可学习
        # 初始化为全0，表示初始时不改变归一化后的值
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        """
        前向传播：应用层归一化

        参数:
            x: 输入张量，形状为(batch_size, seq_len, emb_dim)

        返回:
            归一化后的张量，形状与输入相同

        相关知识：
        - LayerNorm在最后一个维度上计算均值和方差
        - 这与BatchNorm不同，BatchNorm在batch维度上计算
        """
        # ========================================================================
        # 步骤1: 计算均值
        # ========================================================================
        # 在最后一个维度(emb_dim)上计算均值
        # keepdim=True保持维度不变，方便后续广播操作
        # 形状: (batch_size, seq_len, 1)
        mean = x.mean(dim=-1, keepdim=True)

        # ========================================================================
        # 步骤2: 计算方差
        # ========================================================================
        # 在最后一个维度上计算方差
        # unbiased=False使用有偏估计（分母为n而非n-1）
        # 这是PyTorch LayerNorm的默认做法
        # 形状: (batch_size, seq_len, 1)
        var = x.var(dim=-1, keepdim=True, unbiased=False)

        # ========================================================================
        # 步骤3: 归一化
        # ========================================================================
        # 标准化公式: (x - mean) / sqrt(var + eps)
        # eps防止除以0
        # 结果是均值为0，方差为1的分布
        # 形状: (batch_size, seq_len, emb_dim)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)

        # ========================================================================
        # 步骤4: 应用可学习的缩放和平移
        # ========================================================================
        # affine变换: scale * norm_x + shift
        # 这允许模型学习最佳的归一化范围
        # scale和shift会通过反向传播自动学习
        return self.scale * norm_x + self.shift
